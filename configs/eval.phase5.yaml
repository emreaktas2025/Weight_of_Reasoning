seed: 1337

models:
  - configs/models/pythia-70m.yaml
  - configs/models/pythia-410m.yaml
  - configs/models/llama3-1b.yaml

datasets:
  reasoning:
    gsm8k: {n_cpu: 50, n_gpu: 200}
    strategyqa: {n_cpu: 30, n_gpu: 100}
  control:
    wiki: {n_cpu: 50, n_gpu: 200}
  optional:
    math: {n_cpu: 20, n_gpu: 50}

robustness:
  seeds: [42, 1337, 999]
  temperatures: [0.0, 0.2]
  metric_ablations: [AE, APE, APL, CUD, SIB, FL]

baselines:
  features: [token_len, avg_logprob, perplexity, cot_len]
  model: logistic_regression

patchout:
  k_percent: [5, 10, 20]
  max_time_per_model_min: 10

runtime:
  cpu_time_limit_min: 60
  gpu_time_limit_min: 30
  use_gpu: true
  torch_dtype: bfloat16
  max_ablation_samples: 10
  parallel_workers: 2

cache_dir: cache/activations
output_dir: reports/phase5
plots_dir: reports/figs_paper
splits_dir: reports/splits

# Circuit and threshold reuse
circuit_heads_json: reports/circuits/heads.json
control_thresholds_npz: reports/control_thresholds_phase3.npz

