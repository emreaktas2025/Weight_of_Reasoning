# SmolLM REV Research Pipeline

A complete, reproducible research pipeline for evaluating Reasoning Effort in Language Models (REV) using only the SmolLM model family (135M â†’ 350M â†’ 1.7B).

## ğŸ¯ Overview

This pipeline provides:
- **Automatic iteration management** with immutable experiment folders
- **SmolLM-specific evaluation** (HuggingFaceTB/SmolLM-135M, 350M, 1.7B)
- **REV metric computation** (REV, APL, APE, SIB) with activation capture
- **Statistical analysis** (bootstrap CIs, DeLong tests, partial correlations)
- **Publication-ready figures** with semantic naming conventions
- **Complete reproducibility** with git hash tracking and environment snapshots

## ğŸ“ Project Structure

```
Weight_of_Reasoning/
â”œâ”€â”€ configs/smollm/           # SmolLM-specific configurations
â”‚   â”œâ”€â”€ eval.default.yaml    # Full evaluation config
â”‚   â”œâ”€â”€ eval.debug.yaml      # Debug config (10 samples)
â”‚   â””â”€â”€ models/              # Model specifications
â”œâ”€â”€ data/smollm_ids/         # Frozen dataset ID lists
â”œâ”€â”€ experiments/             # Iteration-based results (immutable)
â”‚   â”œâ”€â”€ iteration_001/       # Each run gets unique folder
â”‚   â”œâ”€â”€ iteration_002/
â”‚   â””â”€â”€ iteration_registry.csv
â”œâ”€â”€ src/wor/smollm/          # SmolLM-specific modules
â”‚   â”œâ”€â”€ loaders.py           # Model and data loaders
â”‚   â”œâ”€â”€ activations.py       # PyTorch hooks for activation capture
â”‚   â”œâ”€â”€ metrics/             # REV, APL, APE, SIB implementations
â”‚   â”œâ”€â”€ eval/                # Evaluation pipeline
â”‚   â””â”€â”€ utils/               # Iteration management
â”œâ”€â”€ scripts/smollm/          # Execution scripts
â””â”€â”€ notebooks/               # Analysis notebooks
```

## ğŸš€ Quick Start

### 1. Debug Run (Recommended First)
```bash
# Quick test with 10 samples per dataset
bash scripts/smollm/run_debug.sh
```

### 2. Full Evaluation
```bash
# Complete evaluation with all 3 SmolLM models
bash scripts/smollm/run_all.sh
```

### 3. Summarize Results
```bash
# Aggregate results and update registry
bash scripts/smollm/summarize_iteration.sh
```

## ğŸ“Š Iteration System

Each experiment run creates an immutable iteration folder:

```
experiments/iteration_001/
â”œâ”€â”€ config_used.yaml         # Exact config snapshot
â”œâ”€â”€ env_info.json           # Environment details
â”œâ”€â”€ timing.json             # Runtime information
â”œâ”€â”€ logs/run.log            # Complete stdout/stderr
â”œâ”€â”€ results/                # Model-specific results
â”‚   â”œâ”€â”€ smollm-135m_metrics.csv
â”‚   â”œâ”€â”€ smollm-350m_metrics.csv
â”‚   â”œâ”€â”€ smollm-1.7b_metrics.csv
â”‚   â”œâ”€â”€ model_results.json
â”‚   â””â”€â”€ aggregate_statistics.json
â”œâ”€â”€ figures/                # Generated plots
â”‚   â”œâ”€â”€ Fig1_scaling__iteration_001.png
â”‚   â”œâ”€â”€ Fig2_temp_robustness__iteration_001.png
â”‚   â””â”€â”€ Fig3_ablation_curve__iteration_001.png
â””â”€â”€ notes.md                # Auto-generated template
```

## ğŸ”§ Configuration

### Model Specifications
```yaml
# configs/smollm/models/smollm-135m.yaml
model_name: HuggingFaceTB/SmolLM-135M
device: cuda
dtype: bfloat16
max_new_tokens: 64
temperature: 0.2
```

### Dataset Configuration
```yaml
# configs/smollm/eval.default.yaml
datasets:
  reasoning:
    gsm8k: 800
    strategyqa: 400
  control:
    wiki: 1200
```

## ğŸ“ˆ Metrics Computed

### Core REV Metrics
- **REV**: Composite reasoning effort score
- **APL**: Activation Path Length
- **APE**: Attention Process Entropy  
- **SIB**: Stability of Intermediate Beliefs

### Statistical Analysis
- **AUROC**: Area under ROC curve
- **Î”AUC**: Improvement over baseline
- **Partial correlations**: Controlling for length/perplexity
- **Bootstrap CIs**: 10,000-sample confidence intervals
- **DeLong test**: Statistical significance testing

## ğŸ¨ Figure Generation

Figures follow semantic naming conventions:

- `Fig1_scaling__{iteration_id}.png`: Î”AUC vs log(model parameters)
- `Fig2_temp_robustness__{iteration_id}.png`: AUROC vs Temperature
- `Fig3_ablation_curve__{iteration_id}.png`: Î”-accuracy vs % heads removed

## âœ… Success Criteria

Automated validation checks:
- âœ… No NaNs across temperatures
- âœ… Monotonic Î”AUC: 135M < 350M < 1.7B
- âœ… Î”AUC â‰¥ +0.05 @ 350M; â‰¥ +0.12 @ 1.7B
- âœ… Partial r(REV|length,ppl) â‰¥ 0.3, p < 0.01
- âœ… Temperature variance â‰¤ 0.03

## ğŸ”„ Reproducibility

### Exact Reproduction
```bash
# Reproduce specific iteration
python -m src.wor.smollm.eval.run_eval --config configs/smollm/eval.default.yaml --reproduce iteration_001
```

### Environment Tracking
Each iteration saves:
- Git commit hash
- Python/CUDA/PyTorch versions
- Hardware configuration
- Exact configuration used

## ğŸ“ Usage Examples

### Run Debug Evaluation
```bash
# Quick test with minimal samples
bash scripts/smollm/run_debug.sh
```

### Run Full Pipeline
```bash
# Complete evaluation
bash scripts/smollm/run_all.sh

# Check results
ls experiments/iteration_001/results/
cat experiments/iteration_001/notes.md
```

### Analyze Results
```bash
# Generate summary
bash scripts/smollm/summarize_iteration.sh

# Check iteration registry
cat experiments/iteration_registry.csv
```

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **GPU Memory Error**
   ```bash
   # Use debug config first
   bash scripts/smollm/run_debug.sh
   ```

2. **HuggingFace Authentication**
   ```bash
   # Set up HF token
   export HUGGINGFACE_HUB_TOKEN=your_token_here
   ```

3. **Missing Dependencies**
   ```bash
   # Install required packages
   pip install torch transformer-lens datasets scikit-learn scipy pandas
   ```

### Debug Mode
The debug configuration (`eval.debug.yaml`) uses:
- 10 samples per dataset
- Single model (135M)
- Reduced memory usage
- Faster execution

## ğŸ“Š Expected Outputs

### Per Iteration
- **Results**: `{model}_{dataset}_metrics.csv`
- **Summary**: `metrics_summary.json`
- **Figures**: Paper-ready plots with semantic naming
- **Notes**: Auto-generated template with metadata

### Global Registry
- **Chronological log**: `experiments/iteration_registry.csv`
- **Best iteration**: Highest Î”AUC across all runs
- **Reproducibility**: Complete experiment history

## ğŸ”¬ Research Workflow

1. **Initial Setup**: Run debug evaluation
2. **Full Evaluation**: Execute complete pipeline
3. **Analysis**: Review results and figures
4. **Iteration**: Modify configs and re-run
5. **Publication**: Use best iteration results

## ğŸ“š Key Files

- `src/wor/smollm/eval/run_eval.py`: Main evaluation pipeline
- `src/wor/smollm/utils/io_utils.py`: Iteration management
- `src/wor/smollm/metrics/rev.py`: REV computation
- `scripts/smollm/run_all.sh`: Full execution script
- `configs/smollm/eval.default.yaml`: Default configuration

## ğŸ¯ Next Steps

After running the pipeline:

1. **Review Results**: Check `experiments/iteration_XXX/notes.md`
2. **Generate Figures**: Run visualization scripts
3. **Analyze Metrics**: Examine partial correlations and effect sizes
4. **Iterate**: Modify configurations and re-run
5. **Publish**: Use best iteration for paper results

---

**Note**: This pipeline is designed for SmolLM models only. For other model families, use the main `src/wor/` evaluation modules.
